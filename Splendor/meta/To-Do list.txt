Making learning easier ideas:
- We should try modifying the board gem limit, as well, and the player gem limit.
    Board limit has been modified
    Implementing infinite player gems right now
- Shortfall vectors - clipping card cost minus effective player gems so the model knows costs.

Reward ideas:
- We can do a negative reward for having the same gems as the enemy player
    Or when the enemy player buys a card that aligns with your gems
- Probably best to do an exponential reward based on similarity to top similar cards

Improving the state vector ideas:
- Buying what percentage of each gem.  have a dimension 6 vector and just have that be the percentage of gems purchased.
- *preposterous* but setting a couple specific weights ahead?  I'm struggling to believe that adding in all the
    dimensions that I want to add is a good idea.  It very well could be, but could also be accomplished so easily 
    by changing just a couple biases.  I suppose this gets entirely overwritten by training?  Unlearnable then?

Hyperparameter:
- Confirm Huber vs MSE, and keep this in mind for later.
- Replay buffer is definitely going to stay a constant concern.  Needs calculated often.


After we're done:
- Intelligent loss: When a move to purchase something was good, it means that moves with a high similarity must also be good.  That is, if I take 1 red, 1 blue, and 1 brown, and this was truly the best move, then taking any 2 of those three is *likely* to be good.  Enough so that providing a loss signal of at least 1/10 that value early on in training will be helpful.