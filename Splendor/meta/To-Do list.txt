Making learning easier ideas:
- comment in run.py.  Model is learning to play when moves are random.
- Make dones 1 instead of True because they're numbers in the end anyway?
- We should try modifying the board gem limit, as well, and the player gem limit.
    Board limit has been modified
    Implementing infinite player gems right now
- Shortfall vectors - clipping card cost minus effective player gems so the model knows costs.

Reward ideas:
- We can do a negative reward for having the same gems as the enemy player
    Or when the enemy player buys a card that aligns with your gems
- Probably best to do an exponential reward based on similarity to top similar cards

Improving the state vector ideas:
- Buying what percentage of each gem.  have a dimension 6 vector and just have that be the percentage of gems purchased.
- Include enemy cards purchasable next turn.  The active player should also have the cards that they can afford, but for the following turn... within a range of gems?  As in, legal within any legal take possibility this turn?

Hyperparameter:
- Confirm Huber vs MSE, and keep this in mind for later.
- Replay buffer is definitely going to stay a constant concern.  Needs calculated often.


After we're done:
- Intelligent loss: When a move to purchase something was good, it means that moves with a high similarity must also be good.  That is, if I take 1 red, 1 blue, and 1 brown, and this was truly the best move, then taking any 2 of those three is *likely* to be good.  Enough so that providing a loss signal of at least 1/10 that value early on in training will be helpful.